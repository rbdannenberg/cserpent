# straightforward LL(1) tokenizing cuz no support for regex yet qwq
require "utils"


TAB_SIZE = 4


INLINE_WHITESPACE = [" ", "\t", "\v", "\r", "\f"]


TK_KEYWORD = "KEYWORD"
TK_IDENTIFIER = "IDENTIFIER"
TK_SYMBOL = "SYMBOL"
TK_LITERAL = "LITERAL"
TK_WHITESPACE = "WHITESPACE"
TK_INDENT = "INDENT"
TK_DEDENT = "DEDENT"
TK_BREAK = "LINEBREAK"


KW_STR = [
    "class",
    "var",
    "def",
    "if",
    "elif",
    "else",
    "while",
    "print",
    "for",
    "in",
    "from",
    "to",
    "by",
    "return",
    "load",
    "or",
    "and",
    "is",
    "not",
    "in"
]

LITERAL_STR = ["true", "false", "nil"]

SYMBOL_STR = [
    "**", "<<", ">>", "<=", "!=", ">=",
    "(", ")", "[", "]", "{", "}",
    ":", ",", "=",
    "<", ">",
    "+", "-", "|", "^", 
    "*", "/", "%", "&", "~", "."
]


class Token:

    var tk_type
    var content
    var range

    def init(tk_type, content, range):
        this.tk_type = tk_type
        this.content = content
        this.range = range


# Since `continue` doesn't exist in Serpent, we are using boxed value (single
# integer inside a singleton list) to pass output from a tokenizing function
# so that everything can be put into a single `if/else` condition and avoid
# a bunch of guards or nested `if/else` or smth.


def tk_iden(source_file, pos, boxed_output):
    print len(source_file.source), pos
    stream = source_file.source
    if not (isalpha(stream[pos]) or stream[pos] == "_"):
        return false
    
    ptr = pos + 1
    while ptr < len(stream) and isalnum(stream[ptr]) or stream[ptr] == "_":
        ptr = ptr + 1
    
    boxed_output[0] = ptr
    return true


def tk_symbol(source_file, pos, boxed_output):
    stream = source_file.source
    for s in SYMBOL_STR:
        if s == subseq(stream, pos, pos + len(s)):
            boxed_output[0] = pos + len(s)
            return true

    return false


def tk_whitespace(source_file, pos, boxed_output):
    stream = source_file.source
    if stream[pos] not in INLINE_WHITESPACE:
        return false
    
    for i = pos + 1 to len(stream):
        if stream[i] not in INLINE_WHITESPACE:
            boxed_output[0] = i
            return true
        
    boxed_output[0] = len(stream)
    return true


# tokenizes a comment up to (excluding) the next line break
def tk_comment(source_file, pos, boxed_output):
    stream = source_file.source
    if not (stream[pos] == "#" or subseq(stream, pos, pos + 2) == "//"):
        return false
        
    for i = pos + 1 to len(stream):
        if stream[i] == "\n":
            boxed_output[0] = i
            return true

    boxed_output[0] = len(stream)
    return true


def tk_dec(source_file, pos, boxed_output):
    stream = source_file.source
    if not isdigit(stream[pos]):
        return false
    
    while isdigit(stream[pos]):
        pos = pos + 1

    if stream[pos] == ".":
        pos = pos + 1
        while isdigit(stream[pos]):
            pos = pos + 1
        
    if stream[pos] == "e":
        pos = pos + 1
        if not isdigit(stream[pos]):
            return false
        
        while isdigit(stream[pos]):
            pos = pos + 1
    
    boxed_output[0] = pos
    return true


def tk_hex(source_file, pos, boxed_output):
    stream = source_file.source
    header = subseq(stream, pos, pos + 2)
    if header != "0x" and header != "0X":
        return false

    pos = pos + 2
    while isalnum(stream[pos]):
        pos = pos + 1
    
    boxed_output[0] = pos
    return true


def tk_number(source_file, pos, boxed_output):
    return tk_hex(source_file, pos, boxed_output) or tk_dec(source_file, pos, boxed_output)


def strip_empty_lines(source_file, pos):
    stream = source_file.source
    ptr = pos
    while ptr < len(stream):
        line_ptr = ptr

        # clear preceding whitespaces
        flag = true
        in_comment = false
        while ptr < len(stream) and flag:
            char = stream[ptr]

            # hashtag comment
            if char == "#":
                in_comment = true

            # slash comment
            if ptr < len(stream) - 1 and char == "/" and stream[ptr + 1] == "/":
                in_comment = true

            # end search on linebreak
            if char == "\n":
                flag = false
                ptr = ptr - 1
            
            # end search on non-whitespace if not in comment
            elif char not in INLINE_WHITESPACE and not in_comment:
                flag = false
                ptr = ptr - 1

            ptr = ptr + 1
        
        # return EOF position if file ended
        if ptr == len(stream):
            return ptr

        if stream[ptr] == "\n":
            # skip linebreak if line is empty
            ptr = ptr + 1
        else:
            # return posiion of <last linebreak + 1>
            return line_ptr


def indent_size(source_file, start, end):
    if start == end:
        return res(0)

    stream = source_file.source
    raw = subseq(stream, start, end)
    tabs = raw.count("\t")
    spaces = raw.count(" ")

    if tabs != 0 and spaces != 0:
        return err_res(
            "cannot use mixed space and tabs in indentation",
            source_file.get_range(start, end)
        )

    # true if whitespace contains anything other than " " and "\t"
    if tabs + spaces != end - start:
        return err_res(
            "cannot use whitespace in indentation other than space or tab",
            source_file.get_range(start, end)
        )

    if spaces % TAB_SIZE != 0:
        return err_res(
            "space indentation must be a multiple of " + str(TAB_SIZE) +
            ", got " + str(spaces),
            source_file.get_range(start, end)
        )

    return res(int(spaces / TAB_SIZE) + tabs)


def tokenize(source_file):
    pos = 0
    indent_level = 0
    newline = true # true if a linebreak was just encountered

    # List<Token>
    tokens = []
    stream = source_file.source

    while pos < len(stream):

        # skip empty lines (that doesn't contribute to indentation levels)
        # TODO: find out why hashtag isnt skipped
        if newline:
            pos = strip_empty_lines(source_file, pos)
            if pos == len(stream):
                for i = 0 to indent_level:
                    tokens.append(Token(TK_DEDENT, "", range))
                return res(tokens)

            whitespace_end = [-1]
            if not tk_whitespace(source_file, pos, whitespace_end):
                whitespace_end[0] = pos

            res = indent_size(source_file, pos, whitespace_end[0])
            if not res.ok:
                return res
            
            indent = res.result
            range = source_file.get_range(pos, whitespace_end[0])
            if indent < indent_level:
                for i = indent to indent_level:
                    tokens.append(Token(TK_DEDENT, "", range))
            elif indent > indent_level:
                for i = indent_level to indent:
                    tokens.append(Token(TK_INDENT, "", range))

            indent_level = indent
            newline = false

        # ugly boxed value representation for cursor position
        new_pos = [-1]

        if stream[pos] == "\n": # line break
            newline = true
            new_pos[0] = pos + 1
            range = source_file.get_range(pos, pos + 1)
            tokens.append(Token(TK_BREAK, "", range))

        elif tk_whitespace(source_file, pos, new_pos): # whitespace
            pass = 1

        elif tk_comment(source_file, pos, new_pos): # comment
            pass = 1

        elif tk_number(source_file, pos, new_pos): # nnumeric literal
            lit = subseq(stream, pos, new_pos[0])
            range = source_file.get_range(pos, new_pos[0])
            tokens.append(Token(TK_LITERAL, lit, range))

        elif tk_iden(source_file, pos, new_pos): # identifier
            iden = subseq(stream, pos, new_pos[0])
            tk_type = TK_IDENTIFIER

            if iden in KW_STR:
                tk_type = TK_KEYWORD
            elif iden in LITERAL_STR:
                ty_type = TK_LITERAL

            range = source_file.get_range(pos, new_pos[0])
            tokens.append(Token(tk_type, iden, range))

        if new_pos[0] == -1:
            range = source_file.get_range(pos, pos + 1)
            return err_res(
                "invalid character during tokenizing: " + stream[pos],
                range
            )

        pos = new_pos[0]

    for i = 0 to indent_level:
        tokens.append(Token(TK_DEDENT, "", range))
    return res(tokens)


def pretty_print_tokenize_result(result):
    if result.ok:
        print "Sucessful. Tokens:"
        for tok in result.result:
            print "[" + tok.tk_type +"]", tok.content
    else:
        err = result.error
        range_str = str(err.range.start) + "-" + str(err.range.end)
        print "Tokenizer Error (" + range_str + ")"
        print err.msg
